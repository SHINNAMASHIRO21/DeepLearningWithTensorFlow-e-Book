{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Datasets \n",
    "\n",
    "## 1. feed_dict (Feeding data)\n",
    "   Data is provided while running each step, using 'feed_dict()' argument in the '.run()' or '.eval()' function call. this methos is allows us to pass numpy array of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeding Data\n",
    "y = tf.placeholder(tf.float32)                      # placeholder\n",
    "x = tf.placeholder(tf.float32)                      # placeholder\n",
    "\n",
    "\n",
    "with tf.Session as sess:\n",
    "    X_Array = \n",
    "    Y_Array = \n",
    "    loss = \n",
    "    \n",
    "    sess.run(loss, feed_dict = {x: X_Array, y: Y_Array})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. from files\n",
    "\n",
    "This methos is used when the dataset is very large to ensure that not all data occoupies the memory at once (imagine the 60GB YouTube-8m datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. A list of filename is created using either Tensor \n",
    "[\"file0\", \"File1\"]\n",
    "# or\n",
    "[(\"file%d\"i) for in range(2)]\n",
    "# or using\n",
    "files = tf.train.match_filename_onces('*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Filename Queue using tf.train.string_input_producer()\n",
    "filename_queue = tf.train.string_input_producer(FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Reader \n",
    "# is defined and used to read from files from the filename queue.\n",
    "# THIS READ METHOD a key identify the file and record \n",
    "# (USEFUL WHILE DEBUGING) and scalar string\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(FILE_NAMEQUEUE / DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DECODER\n",
    "# one or more decoder and conversion OPs are then used to decode the 'value'\n",
    "# string into Tensors that make up the training example:\n",
    "record_defaults = [[1], [1], [1]]\n",
    "col1, col2, col3 = tf.decode_csv(value, record_defaults=record_defaults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preloader Data\n",
    "\n",
    "Used when the dataset is small and can be loaded fully in the memory. in this case we can store data into variable or constant.\n",
    "\n",
    "- When using variable\n",
    "we need to set the trainable flag to False so that the data does not change while trainging.\n",
    "- When using constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preloader data as Constant\n",
    "traning_data = ...\n",
    "training_labels = ...\n",
    "\n",
    "with tf.Session as sess:\n",
    "    x_data = tf.constant(training_data)\n",
    "    y_daya = tf.constant(training_labels)\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    \n",
    "# Preloader data as Variables\n",
    "training_data = ...\n",
    "training_labels = ...\n",
    "\n",
    "with tf.Session as sess:\n",
    "    data_x = tf.placeholder(dtype=training_data.type, shape=training_data.shape)\n",
    "    data_y = tf.placeholder(dtype=training_label.dtype, shape=training_label.shape)\n",
    "    x_data = tf.Variable(data_x, trainable=False, collections[])\n",
    "    Y_data = tf.Variable(data_y, trainable=False, collections[])\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining the Model\n",
    "\n",
    "Defining the model is used computational graph is build describing the network structure. it involves specifying hyper parameters, and placeholder sequence in wich information flows from one set of neirons to another and a loss/error function. NEXT CHAPTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training or Learning\n",
    "\n",
    "In DNNs is normaly bases on the gradient descent algorithm, (it will be dealt in detail in chapter 2, Regresssion) where the aim is to find the <b>training variables(weights/biases)</b> such that the error or loss is minimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Model\n",
    "\n",
    "After Training than is Evaluate the Model using <i><b>.predict()</b></i> on validation data ad test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
