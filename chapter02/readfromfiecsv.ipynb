{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Datasets From File(.csv)\n",
    "\n",
    "For this exampel we use datasets from : \n",
    "http://lib.stat.cmu.edu/datasets/\n",
    "there was contains 506 sample cases and each house is assigned 14 attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules required and declare global variable\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use global parameters\n",
    "DATA_FILE = 'boston_housing.csv'\n",
    "BATCH_SIZE = 10\n",
    "NUM_FEATURES = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function that will take as arguments the filename and return in \n",
    "# batches equal  to BATCH_SIZE\n",
    "\n",
    "def data_generator(filename):\n",
    "    \"\"\"\n",
    "    Generates Tesors in batches size of Batch_SIZE.\n",
    "    Args: String Tensor\n",
    "    Filename: from wich data is to be read\n",
    "    Returns: Tensors\n",
    "    features_batch and label_batch\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename that f_require and reader:\n",
    "\n",
    "f_queue = tf.train.string_input_procedur(filename)\n",
    "reader = tf.TextLineReader(skip_header_lines=1)                  # Skip the first line(HEADER)\n",
    "_, values = reader().read(f_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the data to use in case data is missing. Decode the .csv and\n",
    "# select the features we need. For example RM, PTRATIO, and LSTAT\n",
    "\n",
    "record_defaults = [[0.0] for _ in range(NUM_FEATURES)]\n",
    "data = tf.decde_csv(value, record_defaults=record_defaults)\n",
    "features = tf.stack(tf.grather_nd(data,[[5], [10], [12]]))\n",
    "labels = data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to generate and use tf.train.shuffle_batch()\n",
    "# for randomly shuffling the tensors.\n",
    "\n",
    "\n",
    "# min number of elements in the queue after a 'dequeuemin_after_dequeue = 10 *'\n",
    "# the maximum number of elements in the queue\n",
    "capacity = 20 * BATCH_SIZE\n",
    "\n",
    "# shufle teh data to generate BATCH_SIZE sample pairs\n",
    "feature_batch, label_batch = tf.train.shuffle_batch([features, label], bacth_size, capacity=capacity, min_after_dequeue=min_after_dequeue)\n",
    "return feature_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another function to generate batcesh size in the session\n",
    "\n",
    "def generate_data(feature_batch, label_batch):\n",
    "    with tf.Session() as sess:\n",
    "        # initialize the queue threads\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for _ in range(5):                                         # Generate 5 batches\n",
    "            features, labels = sess.run([FEATURE_BATCH, LABEL_BATCH])\n",
    "            print(features, \"HI\")\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 2 functions to get the data in batches.\n",
    "# bellow this to print the data, when learning\n",
    "if __name__ =='__main__':\n",
    "    features_batch, label_batch = data_generator([DATA_FILE])\n",
    "    generate_data(feature_batch, label_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.03  17.9   18.8  ]\n",
      " [ 6.762 19.7    9.5  ]\n",
      " [ 6.727 19.     5.29 ]\n",
      " [ 5.813 21.    14.81 ]\n",
      " [ 6.163 18.    11.34 ]\n",
      " [ 5.399 17.9   30.81 ]\n",
      " [ 5.701 21.    18.35 ]\n",
      " [ 5.456 21.    11.69 ]\n",
      " [ 5.841 19.2   11.41 ]\n",
      " [ 5.889 15.2   15.71 ]] HI\n",
      "[[ 5.85  19.2    8.77 ]\n",
      " [ 6.229 20.9   15.55 ]\n",
      " [ 6.232 18.7   12.34 ]\n",
      " [ 6.195 20.9   13.   ]\n",
      " [ 6.121 18.5    8.44 ]\n",
      " [ 6.474 20.9   12.27 ]\n",
      " [ 6.377 15.2   20.45 ]\n",
      " [ 5.933 19.2    9.68 ]\n",
      " [ 5.935 21.     6.58 ]\n",
      " [ 6.012 15.2   12.43 ]] HI\n",
      "[[ 5.731 17.8   13.61 ]\n",
      " [ 5.594 18.9   13.09 ]\n",
      " [ 5.95  21.    27.71 ]\n",
      " [ 5.949 21.     8.26 ]\n",
      " [ 6.273 18.7    6.78 ]\n",
      " [ 5.928 17.8   15.76 ]\n",
      " [ 5.741 19.7   13.15 ]\n",
      " [ 6.245 19.2    7.54 ]\n",
      " [ 6.14  18.7   10.27 ]\n",
      " [ 6.172 15.2   19.15 ]] HI\n",
      "[[ 5.787 16.1   10.24 ]\n",
      " [ 5.888 21.1   14.8  ]\n",
      " [ 6.137 20.9   13.44 ]\n",
      " [ 5.885 18.9    8.79 ]\n",
      " [ 7.185 17.8    4.03 ]\n",
      " [ 6.674 21.    11.98 ]\n",
      " [ 6.625 18.     6.65 ]\n",
      " [ 5.961 19.2    9.88 ]\n",
      " [ 6.458 21.2   12.6  ]\n",
      " [ 6.511 16.8    5.28 ]] HI\n",
      "[[ 5.637 21.2   18.34 ]\n",
      " [ 6.167 20.9   12.33 ]\n",
      " [ 6.015 18.5   12.86 ]\n",
      " [ 5.857 21.2   21.32 ]\n",
      " [ 6.383 17.3    5.77 ]\n",
      " [ 5.813 21.    19.88 ]\n",
      " [ 6.389 18.5    9.62 ]\n",
      " [ 6.009 15.2   13.27 ]\n",
      " [ 5.87  19.1   14.37 ]\n",
      " [ 7.104 18.6    8.05 ]] HI\n"
     ]
    }
   ],
   "source": [
    "# Global parameters\n",
    "DATA_FILE = 'boston_housing.csv'\n",
    "BATCH_SIZE = 10\n",
    "NUM_FEATURES = 14\n",
    "\n",
    "def data_generator(filename):\n",
    "    \"\"\"\n",
    "    Generates Tensors in batches of size Batch_SIZE.\n",
    "    Args: String Tensor\n",
    "    Filename from which data is to be read\n",
    "    Returns: Tensors\n",
    "    feature_batch and label_batch\n",
    "    \"\"\"\n",
    "\n",
    "    f_queue = tf.train.string_input_producer(filename)\n",
    "    reader = tf.TextLineReader(skip_header_lines=1) # Skips the first line\n",
    "    _, value = reader.read(f_queue)\n",
    "\n",
    "    # next we declare the default value to use in case a data is missing\n",
    "    record_defaults = [ [0.0] for _ in range(NUM_FEATURES)]\n",
    "    data = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    condition = tf.equal(data[13], tf.constant(50.0))\n",
    "    data = tf.where(condition, tf.zeros(NUM_FEATURES), data[:])\n",
    "    features = tf.stack(tf.gather_nd(data,[[5],[10],[12]]))\n",
    "    label = data[-1]\n",
    "\n",
    "    # minimum number elements in the queue after a dequeue\n",
    "    min_after_dequeue = 10 * BATCH_SIZE\n",
    "    # the maximum number of elements in the queue\n",
    "    capacity = 20 * BATCH_SIZE\n",
    "\n",
    "    # shuffle the data to generate BATCH_SIZE sample pairs\n",
    "    feature_batch, label_batch = tf.train.shuffle_batch([features, label], batch_size=BATCH_SIZE,\n",
    "                                                     capacity=capacity, min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "    return feature_batch, label_batch\n",
    "\n",
    "def generate_data(feature_batch, label_batch):\n",
    "    with tf.Session() as sess:\n",
    "        # intialize the queue threads\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for _ in range(5): # Generate 5 batches\n",
    "            features, labels = sess.run([feature_batch, label_batch])\n",
    "            print (features, \"HI\")\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    feature_batch, label_batch = data_generator([DATA_FILE])\n",
    "    generate_data(feature_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
